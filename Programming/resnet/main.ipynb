{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.2+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bance\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:43<00:00, 3962032.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir,\n",
    "                batch_size,\n",
    "                random_seed=42,\n",
    "                valid_size=0.1,\n",
    "                shuffle=True,\n",
    "                test=False):\n",
    "  \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR10(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = data_loader(data_dir='./data',\n",
    "                                         batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data',\n",
    "                              batch_size=64,\n",
    "                              test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.3039\n",
      "Accuracy of the network on the 5000 validation images: 60.46 %\n",
      "Epoch [2/20], Loss: 2.1776\n",
      "Accuracy of the network on the 5000 validation images: 73.84 %\n",
      "Epoch [3/20], Loss: 0.8367\n",
      "Accuracy of the network on the 5000 validation images: 76.88 %\n",
      "Epoch [4/20], Loss: 0.3339\n",
      "Accuracy of the network on the 5000 validation images: 78.38 %\n",
      "Epoch [5/20], Loss: 1.2090\n",
      "Accuracy of the network on the 5000 validation images: 81.56 %\n",
      "Epoch [6/20], Loss: 0.6356\n",
      "Accuracy of the network on the 5000 validation images: 80.9 %\n",
      "Epoch [7/20], Loss: 0.5067\n",
      "Accuracy of the network on the 5000 validation images: 81.48 %\n",
      "Epoch [8/20], Loss: 0.6326\n",
      "Accuracy of the network on the 5000 validation images: 82.56 %\n",
      "Epoch [9/20], Loss: 0.2083\n",
      "Accuracy of the network on the 5000 validation images: 82.32 %\n",
      "Epoch [10/20], Loss: 0.1300\n",
      "Accuracy of the network on the 5000 validation images: 83.22 %\n",
      "Epoch [11/20], Loss: 0.2671\n",
      "Accuracy of the network on the 5000 validation images: 82.52 %\n",
      "Epoch [12/20], Loss: 0.9314\n",
      "Accuracy of the network on the 5000 validation images: 82.24 %\n",
      "Epoch [13/20], Loss: 0.2482\n",
      "Accuracy of the network on the 5000 validation images: 83.56 %\n",
      "Epoch [14/20], Loss: 0.2642\n",
      "Accuracy of the network on the 5000 validation images: 83.96 %\n",
      "Epoch [15/20], Loss: 0.2776\n",
      "Accuracy of the network on the 5000 validation images: 83.46 %\n",
      "Epoch [16/20], Loss: 0.7592\n",
      "Accuracy of the network on the 5000 validation images: 81.98 %\n",
      "Epoch [17/20], Loss: 0.0557\n",
      "Accuracy of the network on the 5000 validation images: 82.68 %\n",
      "Epoch [18/20], Loss: 0.0101\n",
      "Accuracy of the network on the 5000 validation images: 83.1 %\n",
      "Epoch [19/20], Loss: 0.0195\n",
      "Accuracy of the network on the 5000 validation images: 82.6 %\n",
      "Epoch [20/20], Loss: 0.0231\n",
      "Accuracy of the network on the 5000 validation images: 83.58 %\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (20): Flatten(start_dim=1, end_dim=-1)\n",
      "    (21): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (22): ReLU()\n",
      "    (23): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000] Loss: 2.384449\n",
      "Train Epoch: 1 [6400/50000] Loss: 1.476854\n",
      "Train Epoch: 1 [12800/50000] Loss: 1.581348\n",
      "Train Epoch: 1 [19200/50000] Loss: 1.174192\n",
      "Train Epoch: 1 [25600/50000] Loss: 1.324866\n",
      "Train Epoch: 1 [32000/50000] Loss: 1.284573\n",
      "Train Epoch: 1 [38400/50000] Loss: 1.301047\n",
      "Train Epoch: 1 [44800/50000] Loss: 1.093083\n",
      "\n",
      "Test set: Average loss: 1.1953, Accuracy: 5875/10000 (58.75%)\n",
      "\n",
      "Train Epoch: 2 [0/50000] Loss: 1.288552\n",
      "Train Epoch: 2 [6400/50000] Loss: 0.884159\n",
      "Train Epoch: 2 [12800/50000] Loss: 0.966873\n",
      "Train Epoch: 2 [19200/50000] Loss: 0.929604\n",
      "Train Epoch: 2 [25600/50000] Loss: 1.010588\n",
      "Train Epoch: 2 [32000/50000] Loss: 0.932634\n",
      "Train Epoch: 2 [38400/50000] Loss: 1.013240\n",
      "Train Epoch: 2 [44800/50000] Loss: 1.082579\n",
      "\n",
      "Test set: Average loss: 1.1612, Accuracy: 6039/10000 (60.39%)\n",
      "\n",
      "Train Epoch: 3 [0/50000] Loss: 0.749414\n",
      "Train Epoch: 3 [6400/50000] Loss: 1.058039\n",
      "Train Epoch: 3 [12800/50000] Loss: 1.322908\n",
      "Train Epoch: 3 [19200/50000] Loss: 1.090725\n",
      "Train Epoch: 3 [25600/50000] Loss: 1.132679\n",
      "Train Epoch: 3 [32000/50000] Loss: 0.910626\n",
      "Train Epoch: 3 [38400/50000] Loss: 0.959145\n",
      "Train Epoch: 3 [44800/50000] Loss: 0.950115\n",
      "\n",
      "Test set: Average loss: 0.8477, Accuracy: 7032/10000 (70.32%)\n",
      "\n",
      "Train Epoch: 4 [0/50000] Loss: 0.762801\n",
      "Train Epoch: 4 [6400/50000] Loss: 0.788679\n",
      "Train Epoch: 4 [12800/50000] Loss: 0.625820\n",
      "Train Epoch: 4 [19200/50000] Loss: 0.658540\n",
      "Train Epoch: 4 [25600/50000] Loss: 0.720443\n",
      "Train Epoch: 4 [32000/50000] Loss: 0.619596\n",
      "Train Epoch: 4 [38400/50000] Loss: 0.962029\n",
      "Train Epoch: 4 [44800/50000] Loss: 0.811312\n",
      "\n",
      "Test set: Average loss: 0.8505, Accuracy: 7102/10000 (71.02%)\n",
      "\n",
      "Train Epoch: 5 [0/50000] Loss: 0.698233\n",
      "Train Epoch: 5 [6400/50000] Loss: 0.746671\n",
      "Train Epoch: 5 [12800/50000] Loss: 0.699977\n",
      "Train Epoch: 5 [19200/50000] Loss: 0.757380\n",
      "Train Epoch: 5 [25600/50000] Loss: 0.574230\n",
      "Train Epoch: 5 [32000/50000] Loss: 0.686040\n",
      "Train Epoch: 5 [38400/50000] Loss: 0.816087\n",
      "Train Epoch: 5 [44800/50000] Loss: 0.784068\n",
      "\n",
      "Test set: Average loss: 0.6849, Accuracy: 7627/10000 (76.27%)\n",
      "\n",
      "Train Epoch: 6 [0/50000] Loss: 0.596600\n",
      "Train Epoch: 6 [6400/50000] Loss: 0.733057\n",
      "Train Epoch: 6 [12800/50000] Loss: 0.445653\n",
      "Train Epoch: 6 [19200/50000] Loss: 0.572680\n",
      "Train Epoch: 6 [25600/50000] Loss: 0.691704\n",
      "Train Epoch: 6 [32000/50000] Loss: 0.844667\n",
      "Train Epoch: 6 [38400/50000] Loss: 0.496767\n",
      "Train Epoch: 6 [44800/50000] Loss: 0.565645\n",
      "\n",
      "Test set: Average loss: 0.7073, Accuracy: 7559/10000 (75.59%)\n",
      "\n",
      "Train Epoch: 7 [0/50000] Loss: 0.640160\n",
      "Train Epoch: 7 [6400/50000] Loss: 0.677591\n",
      "Train Epoch: 7 [12800/50000] Loss: 0.520918\n",
      "Train Epoch: 7 [19200/50000] Loss: 0.583126\n",
      "Train Epoch: 7 [25600/50000] Loss: 0.456969\n",
      "Train Epoch: 7 [32000/50000] Loss: 0.734596\n",
      "Train Epoch: 7 [38400/50000] Loss: 0.776772\n",
      "Train Epoch: 7 [44800/50000] Loss: 0.516980\n",
      "\n",
      "Test set: Average loss: 0.7233, Accuracy: 7486/10000 (74.86%)\n",
      "\n",
      "Train Epoch: 8 [0/50000] Loss: 0.648091\n",
      "Train Epoch: 8 [6400/50000] Loss: 0.688493\n",
      "Train Epoch: 8 [12800/50000] Loss: 0.598215\n",
      "Train Epoch: 8 [19200/50000] Loss: 0.593192\n",
      "Train Epoch: 8 [25600/50000] Loss: 0.778989\n",
      "Train Epoch: 8 [32000/50000] Loss: 0.384316\n",
      "Train Epoch: 8 [38400/50000] Loss: 0.411739\n",
      "Train Epoch: 8 [44800/50000] Loss: 0.839599\n",
      "\n",
      "Test set: Average loss: 0.6197, Accuracy: 7828/10000 (78.28%)\n",
      "\n",
      "Train Epoch: 9 [0/50000] Loss: 0.684571\n",
      "Train Epoch: 9 [6400/50000] Loss: 0.521165\n",
      "Train Epoch: 9 [12800/50000] Loss: 0.394014\n",
      "Train Epoch: 9 [19200/50000] Loss: 0.520133\n",
      "Train Epoch: 9 [25600/50000] Loss: 0.528637\n",
      "Train Epoch: 9 [32000/50000] Loss: 0.570992\n",
      "Train Epoch: 9 [38400/50000] Loss: 0.422293\n",
      "Train Epoch: 9 [44800/50000] Loss: 0.544600\n",
      "\n",
      "Test set: Average loss: 0.6706, Accuracy: 7773/10000 (77.73%)\n",
      "\n",
      "Train Epoch: 10 [0/50000] Loss: 0.480923\n",
      "Train Epoch: 10 [6400/50000] Loss: 0.614384\n",
      "Train Epoch: 10 [12800/50000] Loss: 0.444119\n",
      "Train Epoch: 10 [19200/50000] Loss: 0.537691\n",
      "Train Epoch: 10 [25600/50000] Loss: 0.559142\n",
      "Train Epoch: 10 [32000/50000] Loss: 0.871633\n",
      "Train Epoch: 10 [38400/50000] Loss: 0.535538\n",
      "Train Epoch: 10 [44800/50000] Loss: 0.445697\n",
      "\n",
      "Test set: Average loss: 0.5995, Accuracy: 7967/10000 (79.67%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Define the network using nn.Sequential\n",
    "        self.network = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Fourth convolutional layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Fifth convolutional layer\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Flatten the output\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Fully connected layers\n",
    "            nn.Linear(512*1*1, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Define a function to train the model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "# Define a function to test the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Data augmentation and normalization for CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the optimizer\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Train and test the model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout(p=0.25, inplace=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout(p=0.25, inplace=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU()\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Dropout(p=0.25, inplace=False)\n",
      "    (20): Flatten(start_dim=1, end_dim=-1)\n",
      "    (21): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (22): ReLU()\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Dropout(p=0.5, inplace=False)\n",
      "    (27): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000] Loss: 2.343339\n",
      "Train Epoch: 1 [12800/50000] Loss: 1.763603\n",
      "Train Epoch: 1 [25600/50000] Loss: 1.814173\n",
      "Train Epoch: 1 [38400/50000] Loss: 1.449557\n",
      "\n",
      "Test set: Average loss: 1.4017, Accuracy: 5164/10000 (51.64%)\n",
      "\n",
      "Train Epoch: 2 [0/50000] Loss: 1.343805\n",
      "Train Epoch: 2 [12800/50000] Loss: 1.490515\n",
      "Train Epoch: 2 [25600/50000] Loss: 1.095617\n",
      "Train Epoch: 2 [38400/50000] Loss: 1.121654\n",
      "\n",
      "Test set: Average loss: 1.1859, Accuracy: 5598/10000 (55.98%)\n",
      "\n",
      "Train Epoch: 3 [0/50000] Loss: 1.233790\n",
      "Train Epoch: 3 [12800/50000] Loss: 1.339350\n",
      "Train Epoch: 3 [25600/50000] Loss: 1.060035\n",
      "Train Epoch: 3 [38400/50000] Loss: 1.258258\n",
      "\n",
      "Test set: Average loss: 0.9638, Accuracy: 6431/10000 (64.31%)\n",
      "\n",
      "Train Epoch: 4 [0/50000] Loss: 1.173520\n",
      "Train Epoch: 4 [12800/50000] Loss: 1.207204\n",
      "Train Epoch: 4 [25600/50000] Loss: 1.112120\n",
      "Train Epoch: 4 [38400/50000] Loss: 1.017357\n",
      "\n",
      "Test set: Average loss: 1.0994, Accuracy: 5979/10000 (59.79%)\n",
      "\n",
      "Train Epoch: 5 [0/50000] Loss: 1.279299\n",
      "Train Epoch: 5 [12800/50000] Loss: 0.996476\n",
      "Train Epoch: 5 [25600/50000] Loss: 1.055867\n",
      "Train Epoch: 5 [38400/50000] Loss: 0.931057\n",
      "\n",
      "Test set: Average loss: 0.8543, Accuracy: 6921/10000 (69.21%)\n",
      "\n",
      "Train Epoch: 6 [0/50000] Loss: 0.966930\n",
      "Train Epoch: 6 [12800/50000] Loss: 0.865421\n",
      "Train Epoch: 6 [25600/50000] Loss: 0.941063\n",
      "Train Epoch: 6 [38400/50000] Loss: 1.045618\n",
      "\n",
      "Test set: Average loss: 0.7522, Accuracy: 7349/10000 (73.49%)\n",
      "\n",
      "Train Epoch: 7 [0/50000] Loss: 1.066470\n",
      "Train Epoch: 7 [12800/50000] Loss: 0.892374\n",
      "Train Epoch: 7 [25600/50000] Loss: 0.845230\n",
      "Train Epoch: 7 [38400/50000] Loss: 0.896667\n",
      "\n",
      "Test set: Average loss: 0.7748, Accuracy: 7327/10000 (73.27%)\n",
      "\n",
      "Train Epoch: 8 [0/50000] Loss: 0.886619\n",
      "Train Epoch: 8 [12800/50000] Loss: 0.973423\n",
      "Train Epoch: 8 [25600/50000] Loss: 0.739348\n",
      "Train Epoch: 8 [38400/50000] Loss: 0.772090\n",
      "\n",
      "Test set: Average loss: 0.7500, Accuracy: 7369/10000 (73.69%)\n",
      "\n",
      "Train Epoch: 9 [0/50000] Loss: 0.763614\n",
      "Train Epoch: 9 [12800/50000] Loss: 0.729972\n",
      "Train Epoch: 9 [25600/50000] Loss: 0.668138\n",
      "Train Epoch: 9 [38400/50000] Loss: 0.781794\n",
      "\n",
      "Test set: Average loss: 0.6731, Accuracy: 7690/10000 (76.90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000] Loss: 0.640466\n",
      "Train Epoch: 10 [12800/50000] Loss: 0.726558\n",
      "Train Epoch: 10 [25600/50000] Loss: 0.801416\n",
      "Train Epoch: 10 [38400/50000] Loss: 0.615673\n",
      "\n",
      "Test set: Average loss: 0.6552, Accuracy: 7679/10000 (76.79%)\n",
      "\n",
      "Train Epoch: 11 [0/50000] Loss: 0.827716\n",
      "Train Epoch: 11 [12800/50000] Loss: 0.728275\n",
      "Train Epoch: 11 [25600/50000] Loss: 0.657450\n",
      "Train Epoch: 11 [38400/50000] Loss: 0.593689\n",
      "\n",
      "Test set: Average loss: 0.5832, Accuracy: 8016/10000 (80.16%)\n",
      "\n",
      "Train Epoch: 12 [0/50000] Loss: 0.475771\n",
      "Train Epoch: 12 [12800/50000] Loss: 0.501783\n",
      "Train Epoch: 12 [25600/50000] Loss: 0.583208\n",
      "Train Epoch: 12 [38400/50000] Loss: 0.631342\n",
      "\n",
      "Test set: Average loss: 0.5742, Accuracy: 8003/10000 (80.03%)\n",
      "\n",
      "Train Epoch: 13 [0/50000] Loss: 0.714715\n",
      "Train Epoch: 13 [12800/50000] Loss: 0.738528\n",
      "Train Epoch: 13 [25600/50000] Loss: 0.588337\n",
      "Train Epoch: 13 [38400/50000] Loss: 0.790604\n",
      "\n",
      "Test set: Average loss: 0.5609, Accuracy: 8113/10000 (81.13%)\n",
      "\n",
      "Train Epoch: 14 [0/50000] Loss: 0.897077\n",
      "Train Epoch: 14 [12800/50000] Loss: 0.573627\n",
      "Train Epoch: 14 [25600/50000] Loss: 0.760264\n",
      "Train Epoch: 14 [38400/50000] Loss: 0.576984\n",
      "\n",
      "Test set: Average loss: 0.5778, Accuracy: 8044/10000 (80.44%)\n",
      "\n",
      "Train Epoch: 15 [0/50000] Loss: 0.596260\n",
      "Train Epoch: 15 [12800/50000] Loss: 0.638639\n",
      "Train Epoch: 15 [25600/50000] Loss: 0.774342\n",
      "Train Epoch: 15 [38400/50000] Loss: 0.693131\n",
      "\n",
      "Test set: Average loss: 0.5558, Accuracy: 8095/10000 (80.95%)\n",
      "\n",
      "Train Epoch: 16 [0/50000] Loss: 0.552026\n",
      "Train Epoch: 16 [12800/50000] Loss: 0.546340\n",
      "Train Epoch: 16 [25600/50000] Loss: 0.608084\n",
      "Train Epoch: 16 [38400/50000] Loss: 0.463972\n",
      "\n",
      "Test set: Average loss: 0.5551, Accuracy: 8108/10000 (81.08%)\n",
      "\n",
      "Train Epoch: 17 [0/50000] Loss: 0.704814\n",
      "Train Epoch: 17 [12800/50000] Loss: 0.832724\n",
      "Train Epoch: 17 [25600/50000] Loss: 0.761238\n",
      "Train Epoch: 17 [38400/50000] Loss: 0.607453\n",
      "\n",
      "Test set: Average loss: 0.5476, Accuracy: 8130/10000 (81.30%)\n",
      "\n",
      "Train Epoch: 18 [0/50000] Loss: 0.636483\n",
      "Train Epoch: 18 [12800/50000] Loss: 0.687603\n",
      "Train Epoch: 18 [25600/50000] Loss: 0.664254\n",
      "Train Epoch: 18 [38400/50000] Loss: 0.412418\n",
      "\n",
      "Test set: Average loss: 0.5477, Accuracy: 8157/10000 (81.57%)\n",
      "\n",
      "Train Epoch: 19 [0/50000] Loss: 0.604464\n",
      "Train Epoch: 19 [12800/50000] Loss: 0.558254\n",
      "Train Epoch: 19 [25600/50000] Loss: 0.487725\n",
      "Train Epoch: 19 [38400/50000] Loss: 0.577089\n",
      "\n",
      "Test set: Average loss: 0.5394, Accuracy: 8187/10000 (81.87%)\n",
      "\n",
      "Train Epoch: 20 [0/50000] Loss: 0.662652\n",
      "Train Epoch: 20 [12800/50000] Loss: 0.676536\n",
      "Train Epoch: 20 [25600/50000] Loss: 0.643758\n",
      "Train Epoch: 20 [38400/50000] Loss: 0.558154\n",
      "\n",
      "Test set: Average loss: 0.5449, Accuracy: 8170/10000 (81.70%)\n",
      "\n",
      "Train Epoch: 21 [0/50000] Loss: 0.482735\n",
      "Train Epoch: 21 [12800/50000] Loss: 0.676468\n",
      "Train Epoch: 21 [25600/50000] Loss: 0.676990\n",
      "Train Epoch: 21 [38400/50000] Loss: 0.794932\n",
      "\n",
      "Test set: Average loss: 0.5363, Accuracy: 8189/10000 (81.89%)\n",
      "\n",
      "Train Epoch: 22 [0/50000] Loss: 0.602289\n",
      "Train Epoch: 22 [12800/50000] Loss: 0.709489\n",
      "Train Epoch: 22 [25600/50000] Loss: 0.625976\n",
      "Train Epoch: 22 [38400/50000] Loss: 0.493353\n",
      "\n",
      "Test set: Average loss: 0.5365, Accuracy: 8186/10000 (81.86%)\n",
      "\n",
      "Train Epoch: 23 [0/50000] Loss: 0.587147\n",
      "Train Epoch: 23 [12800/50000] Loss: 0.600822\n",
      "Train Epoch: 23 [25600/50000] Loss: 0.607014\n",
      "Train Epoch: 23 [38400/50000] Loss: 0.611020\n",
      "\n",
      "Test set: Average loss: 0.5292, Accuracy: 8217/10000 (82.17%)\n",
      "\n",
      "Train Epoch: 24 [0/50000] Loss: 0.527714\n",
      "Train Epoch: 24 [12800/50000] Loss: 0.510706\n",
      "Train Epoch: 24 [25600/50000] Loss: 0.737155\n",
      "Train Epoch: 24 [38400/50000] Loss: 0.501905\n",
      "\n",
      "Test set: Average loss: 0.5268, Accuracy: 8230/10000 (82.30%)\n",
      "\n",
      "Train Epoch: 25 [0/50000] Loss: 0.585140\n",
      "Train Epoch: 25 [12800/50000] Loss: 0.546296\n",
      "Train Epoch: 25 [25600/50000] Loss: 0.614869\n",
      "Train Epoch: 25 [38400/50000] Loss: 0.504756\n",
      "\n",
      "Test set: Average loss: 0.5338, Accuracy: 8192/10000 (81.92%)\n",
      "\n",
      "Train Epoch: 26 [0/50000] Loss: 0.569861\n",
      "Train Epoch: 26 [12800/50000] Loss: 0.558219\n",
      "Train Epoch: 26 [25600/50000] Loss: 0.495284\n",
      "Train Epoch: 26 [38400/50000] Loss: 0.636715\n",
      "\n",
      "Test set: Average loss: 0.5324, Accuracy: 8206/10000 (82.06%)\n",
      "\n",
      "Train Epoch: 27 [0/50000] Loss: 0.712634\n",
      "Train Epoch: 27 [12800/50000] Loss: 0.556775\n",
      "Train Epoch: 27 [25600/50000] Loss: 0.422282\n",
      "Train Epoch: 27 [38400/50000] Loss: 0.633581\n",
      "\n",
      "Test set: Average loss: 0.5287, Accuracy: 8223/10000 (82.23%)\n",
      "\n",
      "Train Epoch: 28 [0/50000] Loss: 0.639699\n",
      "Train Epoch: 28 [12800/50000] Loss: 0.504489\n",
      "Train Epoch: 28 [25600/50000] Loss: 0.637995\n",
      "Train Epoch: 28 [38400/50000] Loss: 0.704499\n",
      "\n",
      "Test set: Average loss: 0.5273, Accuracy: 8221/10000 (82.21%)\n",
      "\n",
      "Train Epoch: 29 [0/50000] Loss: 0.594444\n",
      "Train Epoch: 29 [12800/50000] Loss: 0.590949\n",
      "Train Epoch: 29 [25600/50000] Loss: 0.566133\n",
      "Train Epoch: 29 [38400/50000] Loss: 0.694164\n",
      "\n",
      "Test set: Average loss: 0.5312, Accuracy: 8205/10000 (82.05%)\n",
      "\n",
      "Train Epoch: 30 [0/50000] Loss: 0.524220\n",
      "Train Epoch: 30 [12800/50000] Loss: 0.660588\n",
      "Train Epoch: 30 [25600/50000] Loss: 0.586155\n",
      "Train Epoch: 30 [38400/50000] Loss: 0.554490\n",
      "\n",
      "Test set: Average loss: 0.5277, Accuracy: 8223/10000 (82.23%)\n",
      "\n",
      "Train Epoch: 31 [0/50000] Loss: 0.489865\n",
      "Train Epoch: 31 [12800/50000] Loss: 0.609244\n",
      "Train Epoch: 31 [25600/50000] Loss: 0.584853\n",
      "Train Epoch: 31 [38400/50000] Loss: 0.708745\n",
      "\n",
      "Test set: Average loss: 0.5287, Accuracy: 8220/10000 (82.20%)\n",
      "\n",
      "Train Epoch: 32 [0/50000] Loss: 0.515959\n",
      "Train Epoch: 32 [12800/50000] Loss: 0.510722\n",
      "Train Epoch: 32 [25600/50000] Loss: 0.651016\n",
      "Train Epoch: 32 [38400/50000] Loss: 0.423667\n",
      "\n",
      "Test set: Average loss: 0.5288, Accuracy: 8220/10000 (82.20%)\n",
      "\n",
      "Train Epoch: 33 [0/50000] Loss: 0.461077\n",
      "Train Epoch: 33 [12800/50000] Loss: 0.600822\n",
      "Train Epoch: 33 [25600/50000] Loss: 0.636714\n",
      "Train Epoch: 33 [38400/50000] Loss: 0.461068\n",
      "\n",
      "Test set: Average loss: 0.5294, Accuracy: 8232/10000 (82.32%)\n",
      "\n",
      "Train Epoch: 34 [0/50000] Loss: 0.573990\n",
      "Train Epoch: 34 [12800/50000] Loss: 0.587708\n",
      "Train Epoch: 34 [25600/50000] Loss: 0.444848\n",
      "Train Epoch: 34 [38400/50000] Loss: 0.532542\n",
      "\n",
      "Test set: Average loss: 0.5282, Accuracy: 8212/10000 (82.12%)\n",
      "\n",
      "Train Epoch: 35 [0/50000] Loss: 0.510026\n",
      "Train Epoch: 35 [12800/50000] Loss: 0.568599\n",
      "Train Epoch: 35 [25600/50000] Loss: 0.526123\n",
      "Train Epoch: 35 [38400/50000] Loss: 0.511578\n",
      "\n",
      "Test set: Average loss: 0.5270, Accuracy: 8227/10000 (82.27%)\n",
      "\n",
      "Train Epoch: 36 [0/50000] Loss: 0.633355\n",
      "Train Epoch: 36 [12800/50000] Loss: 0.674127\n",
      "Train Epoch: 36 [25600/50000] Loss: 0.608346\n",
      "Train Epoch: 36 [38400/50000] Loss: 0.558836\n",
      "\n",
      "Test set: Average loss: 0.5279, Accuracy: 8222/10000 (82.22%)\n",
      "\n",
      "Train Epoch: 37 [0/50000] Loss: 0.570860\n",
      "Train Epoch: 37 [12800/50000] Loss: 0.554558\n",
      "Train Epoch: 37 [25600/50000] Loss: 0.559466\n",
      "Train Epoch: 37 [38400/50000] Loss: 0.776994\n",
      "\n",
      "Test set: Average loss: 0.5270, Accuracy: 8211/10000 (82.11%)\n",
      "\n",
      "Train Epoch: 38 [0/50000] Loss: 0.729285\n",
      "Train Epoch: 38 [12800/50000] Loss: 0.494838\n",
      "Train Epoch: 38 [25600/50000] Loss: 0.583499\n",
      "Train Epoch: 38 [38400/50000] Loss: 0.738074\n",
      "\n",
      "Test set: Average loss: 0.5270, Accuracy: 8224/10000 (82.24%)\n",
      "\n",
      "Train Epoch: 39 [0/50000] Loss: 0.601131\n",
      "Train Epoch: 39 [12800/50000] Loss: 0.634885\n",
      "Train Epoch: 39 [25600/50000] Loss: 0.521522\n",
      "Train Epoch: 39 [38400/50000] Loss: 0.682530\n",
      "\n",
      "Test set: Average loss: 0.5264, Accuracy: 8229/10000 (82.29%)\n",
      "\n",
      "Train Epoch: 40 [0/50000] Loss: 0.594902\n",
      "Train Epoch: 40 [12800/50000] Loss: 0.623636\n",
      "Train Epoch: 40 [25600/50000] Loss: 0.716928\n",
      "Train Epoch: 40 [38400/50000] Loss: 0.573115\n",
      "\n",
      "Test set: Average loss: 0.5263, Accuracy: 8234/10000 (82.34%)\n",
      "\n",
      "Train Epoch: 41 [0/50000] Loss: 0.610350\n",
      "Train Epoch: 41 [12800/50000] Loss: 0.525307\n",
      "Train Epoch: 41 [25600/50000] Loss: 0.600186\n",
      "Train Epoch: 41 [38400/50000] Loss: 0.686183\n",
      "\n",
      "Test set: Average loss: 0.5252, Accuracy: 8221/10000 (82.21%)\n",
      "\n",
      "Train Epoch: 42 [0/50000] Loss: 0.598420\n",
      "Train Epoch: 42 [12800/50000] Loss: 0.543913\n",
      "Train Epoch: 42 [25600/50000] Loss: 0.484076\n",
      "Train Epoch: 42 [38400/50000] Loss: 0.485303\n",
      "\n",
      "Test set: Average loss: 0.5264, Accuracy: 8223/10000 (82.23%)\n",
      "\n",
      "Train Epoch: 43 [0/50000] Loss: 0.643539\n",
      "Train Epoch: 43 [12800/50000] Loss: 0.520970\n",
      "Train Epoch: 43 [25600/50000] Loss: 0.663155\n",
      "Train Epoch: 43 [38400/50000] Loss: 0.618996\n",
      "\n",
      "Test set: Average loss: 0.5260, Accuracy: 8225/10000 (82.25%)\n",
      "\n",
      "Train Epoch: 44 [0/50000] Loss: 0.522434\n",
      "Train Epoch: 44 [12800/50000] Loss: 0.607732\n",
      "Train Epoch: 44 [25600/50000] Loss: 0.492655\n",
      "Train Epoch: 44 [38400/50000] Loss: 0.561876\n",
      "\n",
      "Test set: Average loss: 0.5278, Accuracy: 8221/10000 (82.21%)\n",
      "\n",
      "Train Epoch: 45 [0/50000] Loss: 0.541556\n",
      "Train Epoch: 45 [12800/50000] Loss: 0.527051\n",
      "Train Epoch: 45 [25600/50000] Loss: 0.620678\n",
      "Train Epoch: 45 [38400/50000] Loss: 0.626771\n",
      "\n",
      "Test set: Average loss: 0.5279, Accuracy: 8224/10000 (82.24%)\n",
      "\n",
      "Train Epoch: 46 [0/50000] Loss: 0.642738\n",
      "Train Epoch: 46 [12800/50000] Loss: 0.715550\n",
      "Train Epoch: 46 [25600/50000] Loss: 0.602799\n",
      "Train Epoch: 46 [38400/50000] Loss: 0.613733\n",
      "\n",
      "Test set: Average loss: 0.5267, Accuracy: 8215/10000 (82.15%)\n",
      "\n",
      "Train Epoch: 47 [0/50000] Loss: 0.686545\n",
      "Train Epoch: 47 [12800/50000] Loss: 0.506242\n",
      "Train Epoch: 47 [25600/50000] Loss: 0.536087\n",
      "Train Epoch: 47 [38400/50000] Loss: 0.579048\n",
      "\n",
      "Test set: Average loss: 0.5284, Accuracy: 8218/10000 (82.18%)\n",
      "\n",
      "Train Epoch: 48 [0/50000] Loss: 0.690959\n",
      "Train Epoch: 48 [12800/50000] Loss: 0.538876\n",
      "Train Epoch: 48 [25600/50000] Loss: 0.617084\n",
      "Train Epoch: 48 [38400/50000] Loss: 0.596127\n",
      "\n",
      "Test set: Average loss: 0.5252, Accuracy: 8237/10000 (82.37%)\n",
      "\n",
      "Train Epoch: 49 [0/50000] Loss: 0.764173\n",
      "Train Epoch: 49 [12800/50000] Loss: 0.647836\n",
      "Train Epoch: 49 [25600/50000] Loss: 0.757897\n",
      "Train Epoch: 49 [38400/50000] Loss: 0.810983\n",
      "\n",
      "Test set: Average loss: 0.5298, Accuracy: 8211/10000 (82.11%)\n",
      "\n",
      "Train Epoch: 50 [0/50000] Loss: 0.555661\n",
      "Train Epoch: 50 [12800/50000] Loss: 0.479941\n",
      "Train Epoch: 50 [25600/50000] Loss: 0.528694\n",
      "Train Epoch: 50 [38400/50000] Loss: 0.753362\n",
      "\n",
      "Test set: Average loss: 0.5292, Accuracy: 8213/10000 (82.13%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Define the network using nn.Sequential\n",
    "        self.network = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Fourth convolutional layer\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Flatten the output\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Fully connected layers\n",
    "            nn.Linear(512*2*2, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Define a function to train the model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "# Define a function to test the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "# Set up training parameters\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Data augmentation and normalization for CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the optimizer and the learning rate scheduler\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train and test the model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1, 100] loss: 2.304\n",
      "[1, 200] loss: 2.303\n",
      "[1, 300] loss: 2.302\n",
      "[2, 100] loss: 2.302\n",
      "[2, 200] loss: 2.302\n",
      "[2, 300] loss: 2.301\n",
      "[3, 100] loss: 2.301\n",
      "[3, 200] loss: 2.300\n",
      "[3, 300] loss: 2.300\n",
      "[4, 100] loss: 2.299\n",
      "[4, 200] loss: 2.298\n",
      "[4, 300] loss: 2.297\n",
      "[5, 100] loss: 2.295\n",
      "[5, 200] loss: 2.293\n",
      "[5, 300] loss: 2.290\n",
      "[6, 100] loss: 2.279\n",
      "[6, 200] loss: 2.263\n",
      "[6, 300] loss: 2.225\n",
      "[7, 100] loss: 2.084\n",
      "[7, 200] loss: 2.044\n",
      "[7, 300] loss: 2.025\n",
      "[8, 100] loss: 1.976\n",
      "[8, 200] loss: 1.935\n",
      "[8, 300] loss: 1.915\n",
      "[9, 100] loss: 1.865\n",
      "[9, 200] loss: 1.845\n",
      "[9, 300] loss: 1.828\n",
      "[10, 100] loss: 1.814\n",
      "[10, 200] loss: 1.789\n",
      "[10, 300] loss: 1.769\n",
      "[11, 100] loss: 1.726\n",
      "[11, 200] loss: 1.708\n",
      "[11, 300] loss: 1.711\n",
      "[12, 100] loss: 1.669\n",
      "[12, 200] loss: 1.658\n",
      "[12, 300] loss: 1.629\n",
      "[13, 100] loss: 1.595\n",
      "[13, 200] loss: 1.594\n",
      "[13, 300] loss: 1.570\n",
      "[14, 100] loss: 1.544\n",
      "[14, 200] loss: 1.540\n",
      "[14, 300] loss: 1.546\n",
      "[15, 100] loss: 1.523\n",
      "[15, 200] loss: 1.496\n",
      "[15, 300] loss: 1.481\n",
      "[16, 100] loss: 1.495\n",
      "[16, 200] loss: 1.488\n",
      "[16, 300] loss: 1.450\n",
      "[17, 100] loss: 1.448\n",
      "[17, 200] loss: 1.438\n",
      "[17, 300] loss: 1.407\n",
      "[18, 100] loss: 1.394\n",
      "[18, 200] loss: 1.375\n",
      "[18, 300] loss: 1.370\n",
      "[19, 100] loss: 1.359\n",
      "[19, 200] loss: 1.336\n",
      "[19, 300] loss: 1.323\n",
      "[20, 100] loss: 1.309\n",
      "[20, 200] loss: 1.295\n",
      "[20, 300] loss: 1.288\n",
      "[21, 100] loss: 1.255\n",
      "[21, 200] loss: 1.265\n",
      "[21, 300] loss: 1.265\n",
      "[22, 100] loss: 1.237\n",
      "[22, 200] loss: 1.217\n",
      "[22, 300] loss: 1.213\n",
      "[23, 100] loss: 1.193\n",
      "[23, 200] loss: 1.196\n",
      "[23, 300] loss: 1.194\n",
      "[24, 100] loss: 1.163\n",
      "[24, 200] loss: 1.137\n",
      "[24, 300] loss: 1.138\n",
      "[25, 100] loss: 1.127\n",
      "[25, 200] loss: 1.143\n",
      "[25, 300] loss: 1.101\n",
      "[26, 100] loss: 1.098\n",
      "[26, 200] loss: 1.070\n",
      "[26, 300] loss: 1.089\n",
      "[27, 100] loss: 1.066\n",
      "[27, 200] loss: 1.044\n",
      "[27, 300] loss: 1.051\n",
      "[28, 100] loss: 1.052\n",
      "[28, 200] loss: 1.027\n",
      "[28, 300] loss: 1.027\n",
      "[29, 100] loss: 1.005\n",
      "[29, 200] loss: 1.002\n",
      "[29, 300] loss: 0.996\n",
      "[30, 100] loss: 0.983\n",
      "[30, 200] loss: 0.975\n",
      "[30, 300] loss: 0.976\n",
      "[31, 100] loss: 0.966\n",
      "[31, 200] loss: 0.937\n",
      "[31, 300] loss: 0.956\n",
      "[32, 100] loss: 0.919\n",
      "[32, 200] loss: 0.928\n",
      "[32, 300] loss: 0.914\n",
      "[33, 100] loss: 0.924\n",
      "[33, 200] loss: 0.902\n",
      "[33, 300] loss: 0.888\n",
      "[34, 100] loss: 0.878\n",
      "[34, 200] loss: 0.872\n",
      "[34, 300] loss: 0.879\n",
      "[35, 100] loss: 0.850\n",
      "[35, 200] loss: 0.847\n",
      "[35, 300] loss: 0.844\n",
      "[36, 100] loss: 0.841\n",
      "[36, 200] loss: 0.841\n",
      "[36, 300] loss: 0.824\n",
      "[37, 100] loss: 0.798\n",
      "[37, 200] loss: 0.797\n",
      "[37, 300] loss: 0.808\n",
      "[38, 100] loss: 0.786\n",
      "[38, 200] loss: 0.790\n",
      "[38, 300] loss: 0.770\n",
      "[39, 100] loss: 0.764\n",
      "[39, 200] loss: 0.762\n",
      "[39, 300] loss: 0.772\n",
      "[40, 100] loss: 0.785\n",
      "[40, 200] loss: 0.755\n",
      "[40, 300] loss: 0.743\n",
      "[41, 100] loss: 0.748\n",
      "[41, 200] loss: 0.738\n",
      "[41, 300] loss: 0.762\n",
      "[42, 100] loss: 0.732\n",
      "[42, 200] loss: 0.712\n",
      "[42, 300] loss: 0.713\n",
      "[43, 100] loss: 0.708\n",
      "[43, 200] loss: 0.695\n",
      "[43, 300] loss: 0.698\n",
      "[44, 100] loss: 0.701\n",
      "[44, 200] loss: 0.694\n",
      "[44, 300] loss: 0.671\n",
      "[45, 100] loss: 0.692\n",
      "[45, 200] loss: 0.688\n",
      "[45, 300] loss: 0.659\n",
      "[46, 100] loss: 0.663\n",
      "[46, 200] loss: 0.663\n",
      "[46, 300] loss: 0.661\n",
      "[47, 100] loss: 0.640\n",
      "[47, 200] loss: 0.648\n",
      "[47, 300] loss: 0.650\n",
      "[48, 100] loss: 0.638\n",
      "[48, 200] loss: 0.641\n",
      "[48, 300] loss: 0.629\n",
      "[49, 100] loss: 0.627\n",
      "[49, 200] loss: 0.621\n",
      "[49, 300] loss: 0.632\n",
      "[50, 100] loss: 0.613\n",
      "[50, 200] loss: 0.626\n",
      "[50, 300] loss: 0.616\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 77.88%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for the training set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Define transformations for the test set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and test sets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512 * 1 * 1, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        \n",
    "        x = x.view(-1, 512 * 1 * 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = SimpleCNN().to(device)  # Move the model to GPU if available\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
